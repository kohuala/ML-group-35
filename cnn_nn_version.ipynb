{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from skimage.io import imread\n",
    "from skimage.filters import threshold_otsu\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters = [\n",
    "            '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D',\n",
    "            'E', 'F', 'G', 'H','J', 'K', 'L', 'M', 'N','P', 'Q', 'R', 'S', 'T',\n",
    "            'U', 'V', 'W', 'X', 'Y', 'Z'\n",
    "        ]\n",
    "\n",
    "letters_num = [i for i in range(0,len(letters))]\n",
    "\n",
    "dictionary = dict(zip(letters, letters_num))\n",
    "\n",
    "dict2 = dict(zip(letters_num, letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_training_data(training_directory):\n",
    "    image_data =[]\n",
    "    target_data=[]\n",
    "    train_counter = 0\n",
    "    img=[]\n",
    "    amount = 3000\n",
    "    for num in range(amount):\n",
    "        for ltr in letters:\n",
    "            img_path = os.path.join(training_directory, str(ltr), str(ltr) + '_' + str(num) + '.jpg')\n",
    "            if os.path.exists(img_path)==True:\n",
    "                try:\n",
    "                    #print(img_path)\n",
    "                    #img.append(np.array([1,2,3,4]))\n",
    "                    img_details = imread(img_path, as_grey=True)\n",
    "                    binary_image = img_details < threshold_otsu(img_details)\n",
    "                    #print(binary_image.shape, type(binary_image))\n",
    "                    flat_bin_image = binary_image.reshape(-1)\n",
    "                    #flat_bin_image.tolist()\n",
    "                    #print(type(flat_bin_image),flat_bin_image.shape )\n",
    "                    #print(flat_bin_image.shape)\n",
    "                    image_data.append(flat_bin_image)\n",
    "                    #image_data.append(flat_bin_image)\n",
    "                        #print(image_data)\n",
    "                    target_data.append(ltr)\n",
    "    \n",
    "                except OSError as e:\n",
    "                    os.remove(img_path)\n",
    "                    print(\"Removed \"+img_path)\n",
    "            #train_counter+=1\n",
    "    img=np.array(img)\n",
    "    image_data = np.array(image_data)\n",
    "    #image_data=image_data.flatten()\n",
    "    target_data = np.array(target_data)\n",
    "    #print(np.array(image_data).shape)\n",
    "    print(image_data.shape,target_data.shape)       \n",
    "    return (np.array(image_data), np.array(target_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68328,) (68328,)\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "\n",
    "training_dataset_dir = os.path.join(current_dir, 'train/')\n",
    "image_data, target_data = read_training_data(training_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68328, 400)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fff=np.zeros((image_data.shape[0],400))\n",
    "for i in range(image_data.shape[0]):\n",
    "    fff[i,:] = image_data[0]\n",
    "\n",
    "fff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(fff, np.array(target_data), test_size=0.3, random_state=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0],20, 20, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 20, 20,1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47829"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = y_train\n",
    "temp.shape\n",
    "hold=[]\n",
    "for tar in temp:\n",
    "    #v=dict2[np.argmax(array)]\n",
    "    #v=np.argmax(array)\n",
    "    v=dictionary[tar]\n",
    "    hold.append(v)\n",
    "hold\n",
    "\n",
    "temp2 = y_test\n",
    "temp2.shape\n",
    "hold2=[]\n",
    "for array in temp2:\n",
    "    v=dictionary[array]\n",
    "    hold2.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "#Xtrain = (X_train-np.mean(X_train, axis=0))/np.std(X_train, axis=0)\n",
    "#Xtest = (X_test-np.mean(X_train, axis=0))/np.std(X_train, axis=0)\n",
    "# one hot encode outputs\n",
    "Y_train = np_utils.to_categorical(hold)\n",
    "Y_test = np_utils.to_categorical(hold2)\n",
    "num_classes = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5, 5), input_shape=(20,20,1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(64), activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=0.001), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model4():\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3,3),padding='valid', input_shape=(20,20,1), activation='relu'))\n",
    "    model.add(Conv2D(64, (3,3),activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Conv2D(128, (3,3),activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def larger_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(30, (5, 5), input_shape=(20,20,1), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dense(50, activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (5, 5), input_shape=(20,20,1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    #model.add(Conv2D(32, (5, 5), activation='relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fit_model():\n",
    "    # build the model\n",
    "    model = baseline_model()\n",
    "    #model = larger_model()\n",
    "    # Fit the model\n",
    "    cb=EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=50, callbacks=[cb],batch_size=32, verbose=2)\n",
    "    # Final evaluation of the model\n",
    "    scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\n",
    "    \n",
    "    # serialize model to JSON\n",
    "    model_json = model.to_json()\n",
    "    with open(\"models/model_l.json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(\"model_l.h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "    \n",
    "        # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47829 samples, validate on 20499 samples\n",
      "Epoch 1/50\n",
      "16s - loss: 3.4973 - acc: 0.0298 - val_loss: 3.4925 - val_acc: 0.0312\n",
      "Epoch 2/50\n",
      "15s - loss: 3.4952 - acc: 0.0284 - val_loss: 3.4941 - val_acc: 0.0296\n",
      "Epoch 3/50\n",
      "16s - loss: 3.4948 - acc: 0.0328 - val_loss: 3.4927 - val_acc: 0.0301\n",
      "Epoch 4/50\n",
      "15s - loss: 3.4946 - acc: 0.0295 - val_loss: 3.4930 - val_acc: 0.0308\n",
      "Epoch 5/50\n",
      "15s - loss: 3.4947 - acc: 0.0295 - val_loss: 3.4938 - val_acc: 0.0308\n",
      "Epoch 6/50\n",
      "15s - loss: 3.4944 - acc: 0.0307 - val_loss: 3.4917 - val_acc: 0.0300\n",
      "Epoch 7/50\n",
      "14s - loss: 3.4943 - acc: 0.0297 - val_loss: 3.4918 - val_acc: 0.0304\n",
      "Epoch 8/50\n",
      "16s - loss: 3.4942 - acc: 0.0301 - val_loss: 3.4914 - val_acc: 0.0296\n",
      "Epoch 9/50\n",
      "15s - loss: 3.4940 - acc: 0.0305 - val_loss: 3.4918 - val_acc: 0.0308\n",
      "Epoch 10/50\n",
      "15s - loss: 3.4941 - acc: 0.0292 - val_loss: 3.4918 - val_acc: 0.0302\n",
      "Epoch 11/50\n",
      "16s - loss: 3.4938 - acc: 0.0304 - val_loss: 3.4910 - val_acc: 0.0300\n",
      "Epoch 12/50\n",
      "16s - loss: 3.4938 - acc: 0.0297 - val_loss: 3.4923 - val_acc: 0.0304\n",
      "CNN Error: 96.96%\n",
      "Saved model to disk\n",
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-227617332178>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuild_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-a2601a15c2cb>\u001b[0m in \u001b[0;36mbuild_fit_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# summarize history for accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "build_fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
